---
title: "Sales"
author: "Hamdi Ahmed"
date: 2018-07-19T13:46:29+01:00
tags: []
categories: []
---

The first part of the project is to come up with hypotheses about the data. This will help me to think about what I should expect to see from the data given the problem statement. 

>The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.
>
> Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.

## **The Hypotheses**

I came up with the following hypothesis while thinking about the problem. These are just my thoughts and you can come-up with many more of these. Since we're talking about stores and products, lets make different sets for each.

### **Store Level Hypotheses:**

1. **City type:** Stores located in urban or Tier 1 cities should have higher sales because of the higher income levels of people there.
2. **Population Density:** Stores located in densely populated areas should have higher sales because of more demand.
3. **Store Capacity:** Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place
4. **Competitors:** Stores having similar establishments nearby should have less sales because of more competition.
5. **Marketing:** Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.
6. **Location:** Stores located within popular marketplaces should have higher sales because of better access to customers.
7. **Customer Behavior:** Stores keeping the right set of products to meet the local needs of customers will have higher sales.
8. **Ambiance:** Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.
9. **Tax** Stores in jurisdictions with low taxes should have more sales since a lower tax burden should leave more money left over for investmenting in processes which increase sales. 

### **Product Level Hypotheses:**

1. **Brand:** Branded products should have higher sales because of higher trust in the customer.
2. **Packaging:** Products with good packaging can attract customers and sell more.
3. **Utility:** Daily use products should have a higher tendency to sell as compared to the specific use products.
4. **Display Area:** Products which are given bigger shelves in the store are likely to catch attention first and sell more.
5. **Visibility in Store:** The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.
6. **Advertising:** Better advertising of products in the store will should higher sales in most cases.
7. **Promotional Offers:** Products accompanied with attractive offers and discounts will sell more.



## **The data**

``` {r}
# Loading the data

test <- read.csv("Test.csv", stringsAsFactors = FALSE)
train <- read.csv("Train.csv", stringsAsFactors = FALSE)

```

Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe 'joined' with a 'source' column specifying where each observation belongs.

``` {r message = FALSE, echo = FALSE}
library(plyr)
library(dplyr)
library(tidyverse)
library(pastecs)
library(psych)
library(sqldf)
```

``` {r}

#Creating source variable for the train and test data sets

train$source <- "train"
test$source <- "test"

joined <- rbind.fill(train, test)
```

```{r}
dim(train)
```
```{r}
dim(test)
```
```{r}
dim(joined)
```
Thus we can see that "joined" has the same # of columns but rows are equivalent to both test and train. One of the key challenges in any data set is missing values. Lets start by checking which columns contain missing values.


```{r}

map(joined, ~sum(is.na(.)))
```

Note that the Item_Outlet_Sales is the target variable and missing values are ones in the test set. So we need not worry about it. But we'll impute the missing values in Item_Weight and Outlet_Size in the data cleaning section.

Lets look at some basic statistics for numerical variables.

```{r}

summary(joined)

```

Moving to nominal (categorical) variable, lets have a look at the number of unique values in each of them.

```{r}

map(joined, ~sum(!duplicated(.)))

```

This tells us that there are 1559 products and 10 outlets/stores (which was also mentioned in problem statement). Another thing that should catch attention is that Item_Type has 16 unique values. Let's explore further using the frequency of different categories in each nominal variable. I'll exclude the ID and source variables for obvious reasons.

```{r}

joined %>%
  group_by(Item_Fat_Content) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}

joined %>%
  group_by(Item_Type) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}

joined %>%
  group_by(Outlet_Location_Type) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}

joined %>%
  filter(Outlet_Size != "") %>% 
  group_by(Outlet_Size) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

```{r}

joined %>%
  group_by(Outlet_Type) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

The output gives us following observations:

1. **Item_Fat_Content:** Some of ‘Low Fat’ values mis-coded as ‘low fat’ and ‘LF’. Also, some of ‘Regular’ are mentioned as ‘regular’.

2. **Item_Type:** Not all categories have substantial numbers. It looks like combining them can give better results.

3. **Outlet_Type:** Supermarket Type2 and Type3 can be combined. But we should check if that’s a good idea before doing it.

## **Data Cleaning**

This step typically involves imputing missing values and treating outliers. Though outlier removal is very important in regression techniques, advanced tree based algorithms are impervious to outliers. So I’ll leave it to you to try it out. We’ll focus on the imputation step here, which is a very important step.

### **Imputing Missing Values**

We found two variables with missing values – Item_Weight and Outlet_Size. Lets impute the former by the average weight of the particular item. This can be done as:

```{r}

avg_item_weight <- mean(joined$Item_Weight, na.rm = TRUE)

joined[which(is.na(joined$Item_Weight)),"Item_Weight"] <- avg_item_weight

```

Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet.

``` {r message = FALSE, echo = FALSE}
Mode1 <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

```

```{r}
joined[which(joined$Outlet_Size == "" & joined$Outlet_Type == "Grocery Store"), "Outlet_Size" ] <- "Small"

joined[which(joined$Outlet_Size == "" & joined$Outlet_Type == "Supermarket Type1"), "Outlet_Size" ] <- Mode1(joined$Outlet_Size[joined$Outlet_Type == "Supermarket Type1"])

# Mode for each outlet type after imputing

joined %>% 
  group_by(Outlet_Type) %>% 
  summarise(n = Mode1(Outlet_Size))
```

### **Feature Engineering**

We explored some nuances in the data in the data exploration section. Lets move on to resolving them and making our data ready for analysis. We will also create some new variables using the existing ones in this section.

#### **Step 1: Consider combining Outlet_Type**

During exploration, we decided to consider combining the Supermarket Type2 and Type3 variables. But is that a good idea? A quick way to check if that could be is to analyze the mean sales by type of store. If they have similar sales, then keeping them separate won’t help much.

```{r}
# Mean sales for each outlet type

joined %>% 
  group_by(Outlet_Type) %>% 
  summarise(mean_sales = mean(Item_Outlet_Sales, na.rm = TRUE))
```
This shows significant difference between them and we’ll leave them as it is.

#### **Step 2: Modify Item_Visibility**

We noticed that the minimum value here is 0, which makes no practical sense. Let's consider it as missing information and impute it with the mean visibility of that particular product.

```{r}

# Creating an indicator variable showing which have missing values

Ind <- function(t) {
  x <- dim(length(t))
  x[which(t == 0.000000000)] <- 0
  x[which(t != 0.000000000)] <- 1
  return(x)
}

# Adding to the dataset

joined$I <- Ind(joined$Item_Visibility)

# Adding variable for mean item visibility per group

joined$Visibility_Avg <- ave(joined$Item_Visibility, joined$Item_Identifier)

# Replacing those with 0 values with their respective group mean

for ( i in 1:nrow(joined)) {
  if (joined$I[i] == 0)
  {
    joined$Item_Visibility[i] = joined$Visibility_Avg[i]
    
  }
}

```

In step 1 we hypothesized that products with higher visibility are likely to sell more. But along with comparing products on absolute terms, we should look at the visibility of the product in that particular store as compared to the mean visibility of that product across all stores. This will give some idea about how much importance was given to that product in a store as compared to other stores. We can use the ‘Visibility_Avg’ variable made above to achieve this.

```{r}

# Adding Item Visibility mean ratio
joined$Item_Visib_Mean_Ratio <- joined$Item_Visibility/joined$Visibility_Avg

joined %>% 
  summarise(count = sum(!is.na(Item_Visib_Mean_Ratio)),
            mean = mean(Item_Visib_Mean_Ratio, na.rm = TRUE),
            std = sd(Item_Visib_Mean_Ratio, na.rm = TRUE),
            min = min(Item_Visib_Mean_Ratio, na.rm = TRUE),
            `25%`=quantile(Item_Visib_Mean_Ratio, probs=0.25),
            `50%`=quantile(Item_Visib_Mean_Ratio, probs=0.5),
            `75%`=quantile(Item_Visib_Mean_Ratio, probs=0.75),
            max = max(Item_Visib_Mean_Ratio))
```

#### **Step 3: Create a broad category of Type of Item**

Earlier we saw that the Item_Type variable has 16 categories which might prove to be very useful in analysis. So its a good idea to combine them. One way could be to manually assign a new category to each. But there’s a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. So I’ve used the Item_Identifier variable to create a new column:

```{r}

extractCat <- function(cat) {
  cat <- as.character(cat)
  
  if (length(grep("FD", cat)) > 0) {
    return ("Food")
  } else if (length(grep("DR", cat)) > 0) {
    return ("Drinks")
  } else if (length(grep("NC", cat)) > 0) {
    return("Non-Consumables")
  }
}

category <- NULL
for (i in 1:nrow(joined)) {
  category <- c(category, extractCat(joined[i,"Item_Identifier"]))
}

joined$Category <- as.factor(category)

joined %>% 
  group_by(Category) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

```

Another idea could be to combine categories based on sales. The ones with high average sales could be combined together.

```{r}
sales <- NULL
for (i in 1:nrow(joined)) {
  sales <- c(sales,ifelse((joined$Item_Outlet_Sales[i]) < 2181.289, "Low Average Sales", "High Average Sales"))
}

joined$Sales <- as.factor(sales)

joined %>% 
  group_by(Sales) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

```
#### **Step 4: Determine the years of operation of a store**

We wanted to make a new column depicting the years of operation of a store. This can be done as:

```{r}

joined$Outlet_Years <- 2013 - joined$Outlet_Establishment_Year

joined %>% 
  summarise(count = sum(!is.na(Outlet_Years)),
            mean = mean(Outlet_Years, na.rm = TRUE),
            std = sd(Outlet_Years, na.rm = TRUE),
            min = min(Outlet_Years, na.rm = TRUE),
            `25%`=quantile(Outlet_Years, probs=0.25),
            `50%`=quantile(Outlet_Years, probs=0.5),
            `75%`=quantile(Outlet_Years, probs=0.75),
            max = max(Outlet_Years))
```

This shows stores which are 4-28 years old.

#### **Step 5: Modify categories of Item_Fat_Content**

We found typos and difference in representation in categories of Item_Fat_Content variable. This can be corrected as:

```{r}

joined %>% 
  group_by(Item_Fat_Content) %>% 
  summarise(count = n())

joined$Item_Fat_Content <- gsub('LF', 'Low Fat', joined$Item_Fat_Content)
joined$Item_Fat_Content <- gsub('low fat', 'Low Fat', joined$Item_Fat_Content)
joined$Item_Fat_Content <- gsub('reg', 'Regular', joined$Item_Fat_Content)

joined %>% 
  group_by(Item_Fat_Content) %>% 
  summarise(count = n())

```

Now it makes more sense. But hang on, in step 3 we saw there were some non-consumables as well and a fat-content should not be specified for them. So we can also create a separate category for such kind of observations.

```{r}

joined[which(joined$Category == "Non-Consumables"), "Item_Fat_Content"] <- "Non-Edible"

joined %>% 
  group_by(Item_Fat_Content) %>% 
  summarise(count = n())

```